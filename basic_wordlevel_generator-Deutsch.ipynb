{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# GLOVE download reference https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer/blob/master/README.md\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from datetime import date\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense ,Dropout,LSTM, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is a gpu available for training ?\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-alpha0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of unwanted characters\n",
    "badlist = pd.read_csv('data/badlist_de', header=None)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# add spaces to certain special characters\n",
    "unspaced = r'[\\[\\]\\'\\'\\”\\(\\)\\.\\,\\/\\?\\-\\!\\\"\\;|:]'\n",
    "spaced =  ' \\g<0> '\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_line(line):\n",
    "    \"\"\"\n",
    "    remove stray characters\n",
    "    \"\"\"\n",
    "    for badchar in badlist:\n",
    "        if badchar in line:\n",
    "            line = re.sub(badchar,'',line)\n",
    "        line = re.sub(unspaced, spaced, line)\n",
    "        line = re.sub(\"\\n\" ,\" \\n\", line)\n",
    "        for n in range(2):\n",
    "            line = re.sub('  ',' ',line)\n",
    "        \n",
    "    return line\n",
    "\n",
    "def tokenize(sentence):\n",
    "    \"\"\"\n",
    "    simple tokenization keeping the line seperators etc.\n",
    "    \"\"\"\n",
    "    sentence = clean_line(sentence)\n",
    "    return sentence.lower().split(' ') #sentence.lower().split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Warum die royale Hochzeit zum Desaster werden könnte'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_line('Warum die royale Hochzeit zum Desaster werden könnte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read the raw text files and clean the lines\n",
    "\n",
    "all_episodes_by_sentence = []\n",
    "\n",
    "seperators =  [' \\t\\n', '\\t\\n']\n",
    "\n",
    "with open('Newssnippets.txt',encoding='utf-8' ) as in_raw:\n",
    "    # start token\n",
    "    for (i, line) in enumerate(in_raw):\n",
    "        if not line in seperators:\n",
    "            all_episodes_by_sentence.append(clean_line(line) )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Herzogin Meghan Prinzessin Beatrice : Showdown in London : Warum die royale Hochzeit zum Desaster werden könnte \\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_episodes_by_sentence[243]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine text to create a single string for sliceshifting\n",
    "word_dict = {}\n",
    "used_words = []\n",
    "text = []\n",
    "for sent in all_episodes_by_sentence:\n",
    "    for word in tokenize(sent):\n",
    "        #if word in word2idx :\n",
    "        if not word in word_dict:\n",
    "            word_dict[word] = 0\n",
    "        word_dict[word] +=1\n",
    "        text.append(word)\n",
    "#text = [tokenize(sent) for sent in all_episodes_by_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20502"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of words\n",
    "len(word_dict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if we can reduce the number of words\n",
    "data_quant = pd.DataFrame.from_dict(word_dict, orient='index').sort_values(by=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20502, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_quant.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data_quant.tail(50) # looking clean, but many words with only one occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7581, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only 5760 words occur more than 3 times\n",
    "data_quant.loc[data_quant[0] > 2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_vocabulary = data_quant.loc[data_quant[0] > 2].index.tolist()\n",
    "\n",
    "\n",
    "word2idx = {}\n",
    "idx2word = {}\n",
    "for count, word in enumerate(allowed_vocabulary):\n",
    "    word2idx[word] = count\n",
    "    idx2word[count] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.': 0,\n",
       " '\\n': 1,\n",
       " '-': 2,\n",
       " ':': 3,\n",
       " 'der': 4,\n",
       " ',': 5,\n",
       " '2020': 6,\n",
       " 'die': 7,\n",
       " 'und': 8,\n",
       " '02': 9,\n",
       " 'in': 10,\n",
       " 'de': 11,\n",
       " 'mit': 12,\n",
       " 'den': 13,\n",
       " 'im': 14,\n",
       " '03': 15,\n",
       " '(': 16,\n",
       " 'das': 17,\n",
       " ')': 18,\n",
       " 'von': 19,\n",
       " 'ist': 20,\n",
       " 'für': 21,\n",
       " 'auf': 22,\n",
       " 'bundesliga': 23,\n",
       " 'sich': 24,\n",
       " 'ein': 25,\n",
       " 'zu': 26,\n",
       " 'dem': 27,\n",
       " 'am': 28,\n",
       " 'hat': 29,\n",
       " 'freitag': 30,\n",
       " 'schauspieler': 31,\n",
       " 'es': 32,\n",
       " 'bei': 33,\n",
       " 'er': 34,\n",
       " 'film': 35,\n",
       " 'des': 36,\n",
       " 'nach': 37,\n",
       " '?': 38,\n",
       " 'eine': 39,\n",
       " 'donnerstag': 40,\n",
       " 'nicht': 41,\n",
       " 'als': 42,\n",
       " 'mittwoch': 43,\n",
       " '15': 44,\n",
       " 'auch': 45,\n",
       " 'an': 46,\n",
       " '!': 47,\n",
       " 'sänger': 48,\n",
       " 'sie': 49,\n",
       " 'aus': 50,\n",
       " 'gegen': 51,\n",
       " 'dienstag': 52,\n",
       " 'wird': 53,\n",
       " '14': 54,\n",
       " '17': 55,\n",
       " 'vor': 56,\n",
       " 'montag': 57,\n",
       " 'samstag': 58,\n",
       " 'zum': 59,\n",
       " 'florian': 60,\n",
       " '20': 61,\n",
       " 'wie': 62,\n",
       " 'über': 63,\n",
       " '16': 64,\n",
       " 'sonntag': 65,\n",
       " '22': 66,\n",
       " '19': 67,\n",
       " 'einem': 68,\n",
       " '18': 69,\n",
       " 'gala': 70,\n",
       " 'einen': 71,\n",
       " 'stern': 72,\n",
       " '10': 73,\n",
       " 'tv': 74,\n",
       " 'um': 75,\n",
       " 'so': 76,\n",
       " 'seine': 77,\n",
       " '11': 78,\n",
       " '13': 79,\n",
       " 'war': 80,\n",
       " '12': 81,\n",
       " '27': 82,\n",
       " '21': 83,\n",
       " '': 84,\n",
       " '04': 85,\n",
       " '28': 86,\n",
       " 'dass': 87,\n",
       " 'sein': 88,\n",
       " 'noch': 89,\n",
       " 'einer': 90,\n",
       " 'sind': 91,\n",
       " 'online': 92,\n",
       " 'haben': 93,\n",
       " 'nun': 94,\n",
       " '25': 95,\n",
       " 'heidi': 96,\n",
       " 'seiner': 97,\n",
       " '05': 98,\n",
       " 'promi': 99,\n",
       " 'hochzeit': 100,\n",
       " '26': 101,\n",
       " 'the': 102,\n",
       " '2': 103,\n",
       " 'brigitte': 104,\n",
       " '00': 105,\n",
       " 'beim': 106,\n",
       " '23': 107,\n",
       " 'jetzt': 108,\n",
       " '30': 109,\n",
       " '06': 110,\n",
       " 'wieder': 111,\n",
       " 'nur': 112,\n",
       " 'werder': 113,\n",
       " '29': 114,\n",
       " '9': 115,\n",
       " 'star': 116,\n",
       " 'neuen': 117,\n",
       " 'fc': 118,\n",
       " '24': 119,\n",
       " '1': 120,\n",
       " 'ihr': 121,\n",
       " 'fans': 122,\n",
       " 'ihre': 123,\n",
       " 'klum': 124,\n",
       " 'wir': 125,\n",
       " 'fußball': 126,\n",
       " '01': 127,\n",
       " 'ich': 128,\n",
       " 'neue': 129,\n",
       " 's': 130,\n",
       " '0': 131,\n",
       " 'geht': 132,\n",
       " 'wurde': 133,\n",
       " 'mehr': 134,\n",
       " 'was': 135,\n",
       " 'bremen': 136,\n",
       " 'seit': 137,\n",
       " 'vom': 138,\n",
       " 'doch': 139,\n",
       " '8': 140,\n",
       " 'borussia': 141,\n",
       " 'jahren': 142,\n",
       " 'werden': 143,\n",
       " 'schon': 144,\n",
       " 'bvb': 145,\n",
       " 'heute': 146,\n",
       " 'seinen': 147,\n",
       " 'bayern': 148,\n",
       " 'sport1': 149,\n",
       " 'gibt': 150,\n",
       " 'promiflash': 151,\n",
       " 'aber': 152,\n",
       " 'ab': 153,\n",
       " 'zur': 154,\n",
       " 'berlinale': 155,\n",
       " 'trainer': 156,\n",
       " 'zwei': 157,\n",
       " 'kohfeldt': 158,\n",
       " 'jahr': 159,\n",
       " 'dortmund': 160,\n",
       " 'unter': 161,\n",
       " 'spox': 162,\n",
       " '52': 163,\n",
       " 'alle': 164,\n",
       " 'bild': 165,\n",
       " 'serie': 166,\n",
       " 'instagram': 167,\n",
       " 'news': 168,\n",
       " 'berlin': 169,\n",
       " 'us': 170,\n",
       " 'immer': 171,\n",
       " '46': 172,\n",
       " 'diese': 173,\n",
       " 'ihrer': 174,\n",
       " '7': 175,\n",
       " 'gmx': 176,\n",
       " 'mal': 177,\n",
       " 'coronavirus': 178,\n",
       " 'seinem': 179,\n",
       " 'hatte': 180,\n",
       " 'will': 181,\n",
       " 'soll': 182,\n",
       " 'ex': 183,\n",
       " '/': 184,\n",
       " 'ersten': 185,\n",
       " 't': 186,\n",
       " 'wendler': 187,\n",
       " 'durch': 188,\n",
       " 'oder': 189,\n",
       " 'kino': 190,\n",
       " 'steht': 191,\n",
       " 'uhr': 192,\n",
       " 'februar': 193,\n",
       " 'ihren': 194,\n",
       " 'man': 195,\n",
       " 'loomee': 196,\n",
       " 'interview': 197,\n",
       " 'bereits': 198,\n",
       " 'muss': 199,\n",
       " 'märz': 200,\n",
       " 'zwischen': 201,\n",
       " 'kann': 202,\n",
       " 'frankfurt': 203,\n",
       " 'jahre': 204,\n",
       " 'gntm': 205,\n",
       " 'jährige': 206,\n",
       " 'dfb': 207,\n",
       " 'spiel': 208,\n",
       " '6': 209,\n",
       " '38': 210,\n",
       " 'wegen': 211,\n",
       " 'keine': 212,\n",
       " 'wenn': 213,\n",
       " '3': 214,\n",
       " 'macht': 215,\n",
       " 'live': 216,\n",
       " 'zeigt': 217,\n",
       " 'michael': 218,\n",
       " 'bis': 219,\n",
       " 'drei': 220,\n",
       " 'deutschen': 221,\n",
       " '42': 222,\n",
       " 'ganz': 223,\n",
       " 'liebe': 224,\n",
       " 'sehen': 225,\n",
       " 'bekannt': 226,\n",
       " 'kommt': 227,\n",
       " '45': 228,\n",
       " 'zeit': 229,\n",
       " 'staffel': 230,\n",
       " 'kein': 231,\n",
       " 'sagte': 232,\n",
       " 'eintracht': 233,\n",
       " 'große': 234,\n",
       " 'leipzig': 235,\n",
       " 'show': 236,\n",
       " 'at': 237,\n",
       " '50': 238,\n",
       " 'vergangenen': 239,\n",
       " 'spieltag': 240,\n",
       " '31': 241,\n",
       " 'james': 242,\n",
       " 'zurück': 243,\n",
       " '5': 244,\n",
       " 'topmodel': 245,\n",
       " 'beiden': 246,\n",
       " '35': 247,\n",
       " 'deutschland': 248,\n",
       " 'ben': 249,\n",
       " 'weiter': 250,\n",
       " 'regisseur': 251,\n",
       " 'ihrem': 252,\n",
       " '47': 253,\n",
       " 'erste': 254,\n",
       " 'next': 255,\n",
       " 'neuer': 256,\n",
       " 'germany': 257,\n",
       " 'eines': 258,\n",
       " 'leben': 259,\n",
       " 'welt': 260,\n",
       " 'dabei': 261,\n",
       " 'rolle': 262,\n",
       " 'league': 263,\n",
       " 'ins': 264,\n",
       " 'alles': 265,\n",
       " 'deutsche': 266,\n",
       " '90min': 267,\n",
       " 'gab': 268,\n",
       " 'harry': 269,\n",
       " 'bond': 270,\n",
       " 'dieser': 271,\n",
       " 'da': 272,\n",
       " '4': 273,\n",
       " 'sieg': 274,\n",
       " 'hopp': 275,\n",
       " 'diesem': 276,\n",
       " 'spielen': 277,\n",
       " 'song': 278,\n",
       " 'zeiten': 279,\n",
       " 'hier': 280,\n",
       " 'ihn': 281,\n",
       " 'dann': 282,\n",
       " 'münchen': 283,\n",
       " 'freundin': 284,\n",
       " 'video': 285,\n",
       " 'tom': 286,\n",
       " 'köln': 287,\n",
       " 'wer': 288,\n",
       " '40': 289,\n",
       " 'justin': 290,\n",
       " 'uns': 291,\n",
       " 'woche': 292,\n",
       " '59': 293,\n",
       " 'hoffenheim': 294,\n",
       " '2019': 295,\n",
       " 'rb': 296,\n",
       " '43': 297,\n",
       " 'tochter': 298,\n",
       " 'stars': 299,\n",
       " 'erst': 300,\n",
       " '37': 301,\n",
       " 'besten': 302,\n",
       " 'sehr': 303,\n",
       " 'wohl': 304,\n",
       " 'laut': 305,\n",
       " 'eigentlich': 306,\n",
       " 'netflix': 307,\n",
       " '32': 308,\n",
       " '49': 309,\n",
       " 'folge': 310,\n",
       " 'sky': 311,\n",
       " '33': 312,\n",
       " 'pocher': 313,\n",
       " 'foto': 314,\n",
       " 'schauspielerin': 315,\n",
       " 'geburtstag': 316,\n",
       " 'martin': 317,\n",
       " 'tag': 318,\n",
       " 'vier': 319,\n",
       " '34': 320,\n",
       " 'viel': 321,\n",
       " '41': 322,\n",
       " '36': 323,\n",
       " 'hertha': 324,\n",
       " 'spielt': 325,\n",
       " 'dazu': 326,\n",
       " 'tag24': 327,\n",
       " '56': 328,\n",
       " 'silbereisen': 329,\n",
       " 'mönchengladbach': 330,\n",
       " 'ende': 331,\n",
       " 'während': 332,\n",
       " 'laura': 333,\n",
       " 'hollywood': 334,\n",
       " '55': 335,\n",
       " 'perry': 336,\n",
       " 'filme': 337,\n",
       " '39': 338,\n",
       " 'ihm': 339,\n",
       " 'offenbar': 340,\n",
       " 'zweiten': 341,\n",
       " 'spricht': 342,\n",
       " 'gut': 343,\n",
       " 'frau': 344,\n",
       " '48': 345,\n",
       " 'letzten': 346,\n",
       " 'platz': 347,\n",
       " '53': 348,\n",
       " 'großen': 349,\n",
       " 'dieses': 350,\n",
       " 'fan': 351,\n",
       " 'neues': 352,\n",
       " 'zeitung': 353,\n",
       " '51': 354,\n",
       " 'album': 355,\n",
       " 'wochen': 356,\n",
       " 'magazin': 357,\n",
       " 'selbst': 358,\n",
       " 'denn': 359,\n",
       " 'bald': 360,\n",
       " 'sagt': 361,\n",
       " 'damit': 362,\n",
       " '09': 363,\n",
       " 'viele': 364,\n",
       " 'prinz': 365,\n",
       " 'können': 366,\n",
       " 'hält': 367,\n",
       " '08': 368,\n",
       " 'mann': 369,\n",
       " 'wollen': 370,\n",
       " 'sängerin': 371,\n",
       " 'partie': 372,\n",
       " 'model': 373,\n",
       " 'habe': 374,\n",
       " 'fest': 375,\n",
       " '+++': 376,\n",
       " 'focus': 377,\n",
       " 'paar': 378,\n",
       " 'pokal': 379,\n",
       " '|': 380,\n",
       " 'ohne': 381,\n",
       " 'ticker': 382,\n",
       " 'team': 383,\n",
       " 'liga': 384,\n",
       " '57': 385,\n",
       " 'bieber': 386,\n",
       " 'sc': 387,\n",
       " 'machen': 388,\n",
       " 'rolling': 389,\n",
       " 'schalke': 390,\n",
       " '58': 391,\n",
       " 'rtl': 392,\n",
       " 'duell': 393,\n",
       " 'warum': 394,\n",
       " 'lange': 395,\n",
       " 'serien': 396,\n",
       " 'feiert': 397,\n",
       " 'oliver': 398,\n",
       " 'sohn': 399,\n",
       " 'corona': 400,\n",
       " 'ok': 401,\n",
       " 'n': 402,\n",
       " 'dance': 403,\n",
       " 'ob': 404,\n",
       " 'of': 405,\n",
       " 'dietmar': 406,\n",
       " 'ndr': 407,\n",
       " 'fast': 408,\n",
       " 'haaland': 409,\n",
       " 'gute': 410,\n",
       " 'katy': 411,\n",
       " 'britische': 412,\n",
       " 'allem': 413,\n",
       " 'kommenden': 414,\n",
       " 'saison': 415,\n",
       " 'umstyling': 416,\n",
       " 'teil': 417,\n",
       " 'stone': 418,\n",
       " '54': 419,\n",
       " 'musste': 420,\n",
       " 'rund': 421,\n",
       " 'hannover': 422,\n",
       " 'big': 423,\n",
       " 'orlando': 424,\n",
       " 'bekommt': 425,\n",
       " 'kicker': 426,\n",
       " 'weitere': 427,\n",
       " '07': 428,\n",
       " 'worden': 429,\n",
       " 'let': 430,\n",
       " '44': 431,\n",
       " 'sv': 432,\n",
       " 'til': 433,\n",
       " 'champions': 434,\n",
       " 'müssen': 435,\n",
       " 'aktuell': 436,\n",
       " 'sommer': 437,\n",
       " 'stehen': 438,\n",
       " 'band': 439,\n",
       " 'nimmt': 440,\n",
       " 'trotz': 441,\n",
       " 'geschichte': 442,\n",
       " 'gladbach': 443,\n",
       " 'baby': 444,\n",
       " 'waren': 445,\n",
       " 'vater': 446,\n",
       " 'du': 447,\n",
       " 'sei': 448,\n",
       " 'allerdings': 449,\n",
       " 'zehn': 450,\n",
       " 'endlich': 451,\n",
       " 'einige': 452,\n",
       " '2018': 453,\n",
       " 'queen': 454,\n",
       " 'neben': 455,\n",
       " 'zusammen': 456,\n",
       " 'polanski': 457,\n",
       " 'kam': 458,\n",
       " 'gehen': 459,\n",
       " 'berliner': 460,\n",
       " 'tour': 461,\n",
       " 'sport': 462,\n",
       " 'verschoben': 463,\n",
       " 'erstmals': 464,\n",
       " 'schweiger': 465,\n",
       " 'bloom': 466,\n",
       " 'alexander': 467,\n",
       " 'konnte': 468,\n",
       " 'union': 469,\n",
       " 'sondern': 470,\n",
       " 'alter': 471,\n",
       " 'affleck': 472,\n",
       " 'nie': 473,\n",
       " 'bleibt': 474,\n",
       " 'geben': 475,\n",
       " 'würde': 476,\n",
       " 'cesar': 477,\n",
       " '”': 478,\n",
       " 'thomas': 479,\n",
       " 'einmal': 480,\n",
       " 'gestorben': 481,\n",
       " 'sieht': 482,\n",
       " 'lang': 483,\n",
       " 'nachdem': 484,\n",
       " 'gemeinsam': 485,\n",
       " 'wochenende': 486,\n",
       " 'etwas': 487,\n",
       " 'berichtet': 488,\n",
       " 'lässt': 489,\n",
       " 'jennifer': 490,\n",
       " '\"': 491,\n",
       " 'scheint': 492,\n",
       " 'fünf': 493,\n",
       " 'st': 494,\n",
       " 'paderborn': 495,\n",
       " 'lassen': 496,\n",
       " 'hinter': 497,\n",
       " 'kandidatinnen': 498,\n",
       " 'a': 499,\n",
       " 'daniel': 500,\n",
       " 'robert': 501,\n",
       " 'verrät': 502,\n",
       " 'kurz': 503,\n",
       " 'jury': 504,\n",
       " 'kommen': 505,\n",
       " 'dafür': 506,\n",
       " 'promis': 507,\n",
       " 'bsc': 508,\n",
       " 'leverkusen': 509,\n",
       " 'polizei': 510,\n",
       " 'könnte': 511,\n",
       " 'geworden': 512,\n",
       " 'mannschaft': 513,\n",
       " 'andere': 514,\n",
       " 'bühne': 515,\n",
       " 'oscar': 516,\n",
       " 'abstiegskampf': 517,\n",
       " 'mädchen': 518,\n",
       " 'findet': 519,\n",
       " 'müller': 520,\n",
       " 'sowie': 521,\n",
       " 'anders': 522,\n",
       " 'meine': 523,\n",
       " 'heißt': 524,\n",
       " 'john': 525,\n",
       " 'wäre': 526,\n",
       " 'spiegel': 527,\n",
       " 'diesen': 528,\n",
       " 'roman': 529,\n",
       " 'derzeit': 530,\n",
       " 'tsg': 531,\n",
       " 'preis': 532,\n",
       " 'meghan': 533,\n",
       " 'gemacht': 534,\n",
       " 'stand': 535,\n",
       " 'letzte': 536,\n",
       " 'goldenen': 537,\n",
       " 'europa': 538,\n",
       " 'anderen': 539,\n",
       " 'sorgt': 540,\n",
       " 'david': 541,\n",
       " 'sollte': 542,\n",
       " 'statt': 543,\n",
       " 'aktionär': 544,\n",
       " 'city': 545,\n",
       " 'zuletzt': 546,\n",
       " 'wollte': 547,\n",
       " 'wurden': 548,\n",
       " 'millionen': 549,\n",
       " '96': 550,\n",
       " 'konzert': 551,\n",
       " 'single': 552,\n",
       " 'peter': 553,\n",
       " 'jedoch': 554,\n",
       " 'seite': 555,\n",
       " 'amazon': 556,\n",
       " 'com': 557,\n",
       " 'euch': 558,\n",
       " 'sarah': 559,\n",
       " 'läuft': 560,\n",
       " 'anna': 561,\n",
       " 'prinzessin': 562,\n",
       " 'erneut': 563,\n",
       " 'weg': 564,\n",
       " 'frage': 565,\n",
       " 'spieler': 566,\n",
       " 'geplant': 567,\n",
       " 'beste': 568,\n",
       " 'volksblatt': 569,\n",
       " 'zeigen': 570,\n",
       " 'gzsz': 571,\n",
       " 'wirklich': 572,\n",
       " 'startet': 573,\n",
       " 'sieben': 574,\n",
       " 'autor': 575,\n",
       " 'kampf': 576,\n",
       " 'tod': 577,\n",
       " 'esc': 578,\n",
       " 'familie': 579,\n",
       " 'einfach': 580,\n",
       " 'zdf': 581,\n",
       " 'hamburg': 582,\n",
       " 'kinder': 583,\n",
       " 'dreharbeiten': 584,\n",
       " 'nichts': 585,\n",
       " 'gleich': 586,\n",
       " 'beziehung': 587,\n",
       " 'coach': 588,\n",
       " 'bären': 589,\n",
       " 'frauen': 590,\n",
       " 'sogar': 591,\n",
       " '+': 592,\n",
       " 'zuvor': 593,\n",
       " 'tot': 594,\n",
       " 'dank': 595,\n",
       " 'ehemalige': 596,\n",
       " 'musik': 597,\n",
       " '70': 598,\n",
       " 'anderem': 599,\n",
       " 'verliebt': 600,\n",
       " 'grund': 601,\n",
       " 'erklärt': 602,\n",
       " 'driest': 603,\n",
       " 'post': 604,\n",
       " 'südtirol': 605,\n",
       " 'aktienanalyse': 606,\n",
       " 'semmelrogge': 607,\n",
       " 'nächste': 608,\n",
       " 'kleine': 609,\n",
       " 'blick': 610,\n",
       " 'samuel': 611,\n",
       " 'trump': 612,\n",
       " 'disney': 613,\n",
       " 'vs': 614,\n",
       " 'erwartet': 615,\n",
       " 'känguru': 616,\n",
       " 'koch': 617,\n",
       " 'beatrice': 618,\n",
       " 'mai': 619,\n",
       " 'co': 620,\n",
       " 'euro': 621,\n",
       " 'max': 622,\n",
       " 'tränen': 623,\n",
       " 'gerade': 624,\n",
       " 'o': 625,\n",
       " 'hsv': 626,\n",
       " 'auftritt': 627,\n",
       " 'zukunft': 628,\n",
       " 'setzt': 629,\n",
       " 'gegenüber': 630,\n",
       " 'matthias': 631,\n",
       " 'kamera': 632,\n",
       " 'jones': 633,\n",
       " 'knapp': 634,\n",
       " 'sex': 635,\n",
       " 'zuschauer': 636,\n",
       " 'comeback': 637,\n",
       " 'fall': 638,\n",
       " 'kaulitz': 639,\n",
       " 'aller': 640,\n",
       " 'gewesen': 641,\n",
       " 'wo': 642,\n",
       " 'november': 643,\n",
       " 'li': 644,\n",
       " 'ja': 645,\n",
       " 'ihres': 646,\n",
       " 'aktuellen': 647,\n",
       " 'veröffentlicht': 648,\n",
       " 'johnny': 649,\n",
       " 'mir': 650,\n",
       " 'welche': 651,\n",
       " 'los': 652,\n",
       " 'profi': 653,\n",
       " 'mutter': 654,\n",
       " 'sucht': 655,\n",
       " 'highlights': 656,\n",
       " 'seines': 657,\n",
       " 'batman': 658,\n",
       " 'weiß': 659,\n",
       " 'verraten': 660,\n",
       " 'teilte': 661,\n",
       " 'burkhard': 662,\n",
       " 'london': 663,\n",
       " 'singer': 664,\n",
       " 'sollen': 665,\n",
       " 'hätte': 666,\n",
       " 'darum': 667,\n",
       " 'namen': 668,\n",
       " 'jahres': 669,\n",
       " 'neu': 670,\n",
       " 'janine': 671,\n",
       " 'hass': 672,\n",
       " 'präsident': 673,\n",
       " 'tagen': 674,\n",
       " 'paris': 675,\n",
       " 'moviepilot': 676,\n",
       " 'love': 677,\n",
       " 'freunde': 678,\n",
       " 'keinen': 679,\n",
       " 'entscheidung': 680,\n",
       " 'drama': 681,\n",
       " 'hamburger': 682,\n",
       " 'aktiencheck': 683,\n",
       " 'premiere': 684,\n",
       " 'moderator': 685,\n",
       " 'erfolg': 686,\n",
       " 'weil': 687,\n",
       " 'masked': 688,\n",
       " 'eigenen': 689,\n",
       " 'lindemann': 690,\n",
       " 'stuttgart': 691,\n",
       " 'liveticker': 692,\n",
       " 'verriet': 693,\n",
       " 'all': 694,\n",
       " 'williams': 695,\n",
       " 'wissen': 696,\n",
       " 'dort': 697,\n",
       " 'top': 698,\n",
       " 'mich': 699,\n",
       " 'stellt': 700,\n",
       " 'guten': 701,\n",
       " 'regie': 702,\n",
       " 'kritik': 703,\n",
       " 'start': 704,\n",
       " 'kaum': 705,\n",
       " 'tages': 706,\n",
       " 'infos': 707,\n",
       " 'folgen': 708,\n",
       " 'vfl': 709,\n",
       " 'talent': 710,\n",
       " 'angst': 711,\n",
       " 'alt': 712,\n",
       " 'dolic': 713,\n",
       " 'darf': 714,\n",
       " 'nächsten': 715,\n",
       " 'bilder': 716,\n",
       " 'pink': 717,\n",
       " 'nürnberg': 718,\n",
       " 'trifft': 719,\n",
       " 'lebens': 720,\n",
       " 'augsburg': 721,\n",
       " 'mäzen': 722,\n",
       " 'daniela': 723,\n",
       " 'sat': 724,\n",
       " 'nahm': 725,\n",
       " 'sturm': 726,\n",
       " 'marvel': 727,\n",
       " 'hailey': 728,\n",
       " 'titel': 729,\n",
       " 'menschen': 730,\n",
       " 'to': 731,\n",
       " 'niederlage': 732,\n",
       " 'bin': 733,\n",
       " 'monaten': 734,\n",
       " 'and': 735,\n",
       " 'lewis': 736,\n",
       " 'thunberg': 737,\n",
       " 'davon': 738,\n",
       " 'tatort': 739,\n",
       " 'übernimmt': 740,\n",
       " 'neuhaus': 741,\n",
       " 'vielen': 742,\n",
       " 'zieht': 743,\n",
       " 'april': 744,\n",
       " 'chris': 745,\n",
       " 'freund': 746,\n",
       " 'fischer': 747,\n",
       " 'schlager': 748,\n",
       " 'stadion': 749,\n",
       " 'bang': 750,\n",
       " 'machte': 751,\n",
       " 'bär': 752,\n",
       " 'ehe': 753,\n",
       " 'trennung': 754,\n",
       " 'gar': 755,\n",
       " 'kapitän': 756,\n",
       " 'nachwuchs': 757,\n",
       " 'erling': 758,\n",
       " 'rassismus': 759,\n",
       " 'klinsmann': 760,\n",
       " 'düsseldorf': 761,\n",
       " 'darauf': 762,\n",
       " 'is': 763,\n",
       " 'tage': 764,\n",
       " 'krise': 765,\n",
       " 'osbourne': 766,\n",
       " 'junge': 767,\n",
       " 'brother': 768,\n",
       " 'hanau': 769,\n",
       " 'b': 770,\n",
       " 'mcu': 771,\n",
       " 'bayer': 772,\n",
       " 'hauptrolle': 773,\n",
       " 'besonders': 774,\n",
       " 'mein': 775,\n",
       " 'ford': 776,\n",
       " 'treffen': 777,\n",
       " 'oft': 778,\n",
       " 'raus': 779,\n",
       " 'steckt': 780,\n",
       " 'schlechte': 781,\n",
       " 'sorgte': 782,\n",
       " 'erstes': 783,\n",
       " 'gegeben': 784,\n",
       " 'ard': 785,\n",
       " 'valentinstag': 786,\n",
       " 'barbara': 787,\n",
       " 'bringt': 788,\n",
       " 'abend': 789,\n",
       " 'finden': 790,\n",
       " 'sterben': 791,\n",
       " 'dfl': 792,\n",
       " 'usa': 793,\n",
       " 'traum': 794,\n",
       " 'twitter': 795,\n",
       " 'einigen': 796,\n",
       " 'klums': 797,\n",
       " 'contest': 798,\n",
       " 'kind': 799,\n",
       " 'anfang': 800,\n",
       " 'bisher': 801,\n",
       " 'klaws': 802,\n",
       " 'zweite': 803,\n",
       " 'till': 804,\n",
       " 'jan': 805,\n",
       " 'depp': 806,\n",
       " 'österreich': 807,\n",
       " 'aktuelle': 808,\n",
       " 'christian': 809,\n",
       " 'films': 810,\n",
       " 'handball': 811,\n",
       " 'verheiratet': 812,\n",
       " 'kiel': 813,\n",
       " 'kennen': 814,\n",
       " 'überraschung': 815,\n",
       " 'dsds': 816,\n",
       " 'postet': 817,\n",
       " 'no': 818,\n",
       " 'herzogin': 819,\n",
       " 'begeistert': 820,\n",
       " 'wechsel': 821,\n",
       " 'harrison': 822,\n",
       " 'präsentiert': 823,\n",
       " 'genau': 824,\n",
       " 'fragen': 825,\n",
       " 'frankfurter': 826,\n",
       " 'abschied': 827,\n",
       " 'charts': 828,\n",
       " 'zumindest': 829,\n",
       " 'bekommen': 830,\n",
       " 'chroniken': 831,\n",
       " 'mark': 832,\n",
       " 'freuen': 833,\n",
       " 'elton': 834,\n",
       " '000': 835,\n",
       " 'feiern': 836,\n",
       " 'zwar': 837,\n",
       " 'vfb': 838,\n",
       " 'gewinnt': 839,\n",
       " 'freiburg': 840,\n",
       " 'klar': 841,\n",
       " 'rennen': 842,\n",
       " 'freitagabend': 843,\n",
       " 'verloren': 844,\n",
       " 'parasite': 845,\n",
       " 'pop': 846,\n",
       " 'öffentlich': 847,\n",
       " 'später': 848,\n",
       " 'ausgezeichnet': 849,\n",
       " 'geheimnis': 850,\n",
       " 'zudem': 851,\n",
       " 'allen': 852,\n",
       " 'angeblich': 853,\n",
       " 'jede': 854,\n",
       " 'story': 855,\n",
       " 'dresden': 856,\n",
       " 'gewinner': 857,\n",
       " '2017': 858,\n",
       " 'januar': 859,\n",
       " 'ozzy': 860,\n",
       " 'darüber': 861,\n",
       " 'bon': 862,\n",
       " 'taylor': 863,\n",
       " 'erzählt': 864,\n",
       " 'chance': 865,\n",
       " 'darsteller': 866,\n",
       " 'kritisiert': 867,\n",
       " 'weit': 868,\n",
       " 'jürgen': 869,\n",
       " 'rückkehr': 870,\n",
       " 'publikum': 871,\n",
       " 'livestream': 872,\n",
       " 'gezeigt': 873,\n",
       " 'quarantäne': 874,\n",
       " 'gast': 875,\n",
       " 'styles': 876,\n",
       " 'indiana': 877,\n",
       " 'also': 878,\n",
       " 'partner': 879,\n",
       " 'dessen': 880,\n",
       " 'mainz': 881,\n",
       " 'spielte': 882,\n",
       " 'sechs': 883,\n",
       " 'horror': 884,\n",
       " 'kämpfen': 885,\n",
       " 'dc': 886,\n",
       " 'pauli': 887,\n",
       " 'dynamo': 888,\n",
       " 'möchte': 889,\n",
       " 'lars': 890,\n",
       " 'weiteren': 891,\n",
       " 'paul': 892,\n",
       " 'ruby': 893,\n",
       " 'jonas': 894,\n",
       " 'vorwürfe': 895,\n",
       " 'verleihung': 896,\n",
       " 'shooting': 897,\n",
       " 'sicher': 898,\n",
       " 'wolfsburg': 899,\n",
       " 'einsatz': 900,\n",
       " 'kämpft': 901,\n",
       " 'gil': 902,\n",
       " 'außerdem': 903,\n",
       " 'billie': 904,\n",
       " 'schweighöfer': 905,\n",
       " 'sueddeutsche': 906,\n",
       " 'stark': 907,\n",
       " 'frank': 908,\n",
       " 'punkte': 909,\n",
       " 'kandidatin': 910,\n",
       " 'entwicklungen': 911,\n",
       " 'eurovision': 912,\n",
       " 'diesjährigen': 913,\n",
       " 'reihe': 914,\n",
       " 'goldener': 915,\n",
       " 'prosieben': 916,\n",
       " 'schrieb': 917,\n",
       " 'chef': 918,\n",
       " 'droht': 919,\n",
       " 'allgemeine': 920,\n",
       " 'roten': 921,\n",
       " 'ihnen': 922,\n",
       " 'runde': 923,\n",
       " 'erklärte': 924,\n",
       " 'erwarten': 925,\n",
       " 'kinos': 926,\n",
       " 'fordert': 927,\n",
       " 'tor': 928,\n",
       " 'meier': 929,\n",
       " 'projekt': 930,\n",
       " 'spiele': 931,\n",
       " 'karriere': 932,\n",
       " 'beginn': 933,\n",
       " 'eins': 934,\n",
       " 'hatten': 935,\n",
       " 'derstandard': 936,\n",
       " 'musiker': 937,\n",
       " 'etwa': 938,\n",
       " 'alex': 939,\n",
       " 'club': 940,\n",
       " 'ofarim': 941,\n",
       " 'teilnahme': 942,\n",
       " 'herz': 943,\n",
       " 'anzeige': 944,\n",
       " 'schreibt': 945,\n",
       " 'topspiel': 946,\n",
       " '100': 947,\n",
       " 'katzenberger': 948,\n",
       " 'jason': 949,\n",
       " 'alte': 950,\n",
       " 'stattfinden': 951,\n",
       " 'situation': 952,\n",
       " 'verlassen': 953,\n",
       " 'gerne': 954,\n",
       " 'erhält': 955,\n",
       " 'ging': 956,\n",
       " 'freut': 957,\n",
       " 'filmemacher': 958,\n",
       " 'klaas': 959,\n",
       " 'friends': 960,\n",
       " 'genug': 961,\n",
       " 'julian': 962,\n",
       " 'playboy': 963,\n",
       " 'absage': 964,\n",
       " 'fällt': 965,\n",
       " 'fee': 966,\n",
       " 'comedian': 967,\n",
       " 'schwer': 968,\n",
       " 'fitz': 969,\n",
       " 'internet': 970,\n",
       " 'viertelfinale': 971,\n",
       " 'heimspiel': 972,\n",
       " 'joko': 973,\n",
       " 'trailer': 974,\n",
       " 'gefeiert': 975,\n",
       " 'wars': 976,\n",
       " 'dieter': 977,\n",
       " 'schlagersänger': 978,\n",
       " 'kehrt': 979,\n",
       " 'land': 980,\n",
       " 'virus': 981,\n",
       " 'tritt': 982,\n",
       " 'filmen': 983,\n",
       " 'abstieg': 984,\n",
       " 'schließlich': 985,\n",
       " 'wort': 986,\n",
       " '80': 987,\n",
       " 'neun': 988,\n",
       " 'castingshow': 989,\n",
       " 'pressekonferenz': 990,\n",
       " 'ehefrau': 991,\n",
       " 'darin': 992,\n",
       " 'klubs': 993,\n",
       " 'karlsruher': 994,\n",
       " 'you': 995,\n",
       " 'vorbei': 996,\n",
       " 'dürfte': 997,\n",
       " 'werner': 998,\n",
       " 'großes': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare readability, create text from only allowed words\n",
    "text = []\n",
    "for sent in all_episodes_by_sentence:\n",
    "    for word in tokenize(sent):\n",
    "        #if word in word2idx :\n",
    "        if word in allowed_vocabulary:\n",
    "            word_dict[word] +=1\n",
    "            text.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Donnerstag 08 . 08 . 2019 17 : 07 - Übermedien \\n',\n",
       " 'Klatschpresse macht Klatschpresse - Opfer zum Jammerlappen \\n',\n",
       " 'Interview und beschwerte sich über die außerordentlichen Zumutungen der Boulevardpresse . Die nahm das als neue Munition , um gegen ihn zu schießen . Weiterlesen auf Übermedien : Klatschpresse macht Klatschpresse - Opfer zum \\n',\n",
       " 'Donnerstag 25 . 07 . 2019 15 : 30 - Übermedien \\n',\n",
       " 'Thomas Seitel im Lügentornado \\n',\n",
       " 'Seit einem halben Jahr ist Thomas Seitel mit Helene Fischer liiert , seitdem wird er Woche für Woche von der Klatschpresse durch den Dreck gezogen . Im Zeit Magazin hat er sich erstmals darüber geäußert , wie belastend die Berichterstattung für ihn \\n',\n",
       " ' Klatschpresse . \\n',\n",
       " 'Donnerstag 16 . 01 . 2020 7 : 16 - GMX \\n',\n",
       " 'Das sagt Hugh Grant über den royalen Rückzug \\n',\n",
       " 'Vor allem die britische Klatschpresse kritisiert Prinz Harry und Herzogin Meghan für ihre Entscheidung . Der Schauspieler hat deswegen eine eindeutige Meinung . \\n']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original text, first 10 sentences\n",
    "all_episodes_by_sentence[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'donnerstag 08 . 08 . 2019 17 : 07 - übermedien \\n klatschpresse macht klatschpresse - opfer zum \\n interview und sich über die der . die nahm das als neue , um gegen ihn zu schießen . weiterlesen auf übermedien : klatschpresse macht klatschpresse - opfer zum \\n donnerstag 25 . 07 . 2019 15 : 30 - übermedien \\n thomas seitel im \\n seit einem halben jahr ist thomas seitel mit helene fischer liiert , seitdem wird er woche für woche von der klatschpresse durch den gezogen . im zeit magazin hat er sich erstmals darüber geäußert , wie die für ihn \\n  klatschpresse . \\n donnerstag 16 . 01 . 2020 7 : 16 - gmx \\n das sagt hugh grant über den rückzug \\n vor allem die britische klatschpresse kritisiert prinz harry und herzogin meghan für ihre entscheidung . der schauspieler hat deswegen eine meinung . \\n freitag 30 . 08 . 2019 18 : 04 - spiegel online \\n und : im hause schweinsteiger - \\n hugh grant ärgert sich über den britischen und helene fischer über die klatschpresse . grund zur freude gibt es hingegen bei und bastian schweinsteiger . die der woche'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(text[0:200]) # missing a few proper nouns like Radiohead, but this should do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7581 unique words\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(allowed_vocabulary)\n",
    "print ('{} unique words'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "# Here we are acutally using the pretrained words, so this is not needed\n",
    "word2idx = {u:i for i, u in enumerate(allowed_vocabulary)}\n",
    "idx2word = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 40, 368,   0, ...,  20,   4,   1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int = np.array([word2idx[c] for c in text])\n",
    "text_as_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length sentence we want for a single input in words\n",
    "seq_length = 30\n",
    "examples_per_epoch = len(text)//(seq_length+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 40, 368,   0, ...,  20,   4,   1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training examples / targets\n",
    "word_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sequences from character dataset\n",
    "sequences = word_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_shift_input(segment):\n",
    "    \"\"\"\n",
    "    Creates the teaching data by shifting the training data on off to create labeled data\n",
    "    \"\"\"\n",
    "    input_segment = segment[:-1]\n",
    "    target_segment = segment[1:]\n",
    "    return input_segment, target_segment\n",
    "\n",
    "dataset = sequences.map(split_shift_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dataset as prebatched\n",
    "BATCH_SIZE = 35\n",
    "\n",
    "# shuffle the dataset and set batch size\n",
    "dataset = dataset.shuffle(10000).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Length of the vocabulary\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# hyperparameters\n",
    "\n",
    "# embedding dimension\n",
    "embedding_dim = 300\n",
    "\n",
    "# RNN units\n",
    "rnn_units = 800\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    \n",
    "    i = Input(shape=(None,), batch_size=batch_size, dtype=tf.int64 )\n",
    "    # use pretrained embedding now\n",
    "    \n",
    "    # here is the new embedding layer\n",
    "\n",
    "    # preset the weights as untrainable\n",
    "    x = Embedding(vocab_size, embedding_dim,  trainable=True)(i)\n",
    "    x = LSTM(rnn_units, \n",
    "             return_sequences=True,\n",
    "             stateful=True)(x)\n",
    "    x = Dense(vocab_size)(x)\n",
    "    x = Dense(vocab_size)(x)\n",
    "\n",
    "    model = Model(i,x)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7fede43fcba8>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(35, None)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (35, None, 300)           2274300   \n",
      "_________________________________________________________________\n",
      "unified_lstm_2 (UnifiedLSTM) (35, None, 800)           3523200   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (35, None, 7581)          6072381   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (35, None, 7581)          57479142  \n",
      "=================================================================\n",
      "Total params: 69,349,023\n",
      "Trainable params: 69,349,023\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# simple model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    \"\"\"\n",
    "    Define loss function \n",
    "    \"\"\"\n",
    "    return sparse_categorical_crossentropy(labels, logits, from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss =loss)# loss='sparse_categorical_crossentropy' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "today = date.today()\n",
    "\n",
    "checkpoint_dir = './pretrained_word_training_checkpoints_{today}'.format(today=today)\n",
    "\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\") # only save last checkpoint\n",
    "\n",
    "# define callbacks\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 4.2307\n",
      "Epoch 2/50\n",
      "248/248 [==============================] - 19s 75ms/step - loss: 3.7350\n",
      "Epoch 3/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 3.3262\n",
      "Epoch 4/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 2.9848\n",
      "Epoch 5/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 2.6886\n",
      "Epoch 6/50\n",
      "248/248 [==============================] - 20s 79ms/step - loss: 2.4199\n",
      "Epoch 7/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 2.1737\n",
      "Epoch 8/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 1.9416\n",
      "Epoch 9/50\n",
      "248/248 [==============================] - 15s 61ms/step - loss: 1.7187\n",
      "Epoch 10/50\n",
      "248/248 [==============================] - 19s 76ms/step - loss: 1.5136\n",
      "Epoch 11/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 1.3245\n",
      "Epoch 12/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 1.1545\n",
      "Epoch 13/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 0.9985\n",
      "Epoch 14/50\n",
      "248/248 [==============================] - 20s 82ms/step - loss: 0.8635\n",
      "Epoch 15/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 0.7512\n",
      "Epoch 16/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 0.6519\n",
      "Epoch 17/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 0.5703\n",
      "Epoch 18/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 0.4966\n",
      "Epoch 19/50\n",
      "248/248 [==============================] - 20s 79ms/step - loss: 0.4336\n",
      "Epoch 20/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 0.3874\n",
      "Epoch 21/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 0.3518\n",
      "Epoch 22/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 0.3246\n",
      "Epoch 23/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 0.2989\n",
      "Epoch 24/50\n",
      "248/248 [==============================] - 18s 73ms/step - loss: 0.2749\n",
      "Epoch 25/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 0.2462\n",
      "Epoch 26/50\n",
      "248/248 [==============================] - 15s 61ms/step - loss: 0.2312\n",
      "Epoch 27/50\n",
      "248/248 [==============================] - 15s 61ms/step - loss: 0.2161\n",
      "Epoch 28/50\n",
      "248/248 [==============================] - 20s 82ms/step - loss: 0.2094\n",
      "Epoch 29/50\n",
      "248/248 [==============================] - 15s 62ms/step - loss: 0.2138\n",
      "Epoch 30/50\n",
      "248/248 [==============================] - 16s 63ms/step - loss: 0.1952\n",
      "Epoch 31/50\n",
      "248/248 [==============================] - 16s 63ms/step - loss: 0.1862\n",
      "Epoch 32/50\n",
      "248/248 [==============================] - 22s 88ms/step - loss: 0.1855\n",
      "Epoch 33/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 0.1849\n",
      "Epoch 34/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 0.1787\n",
      "Epoch 35/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 0.1711\n",
      "Epoch 36/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 0.1663\n",
      "Epoch 37/50\n",
      "248/248 [==============================] - 20s 82ms/step - loss: 0.1566\n",
      "Epoch 38/50\n",
      "248/248 [==============================] - 16s 64ms/step - loss: 0.1632\n",
      "Epoch 39/50\n",
      "248/248 [==============================] - 16s 64ms/step - loss: 0.1640\n",
      "Epoch 40/50\n",
      "248/248 [==============================] - 16s 63ms/step - loss: 0.1595\n",
      "Epoch 41/50\n",
      "248/248 [==============================] - 22s 89ms/step - loss: 0.1610\n",
      "Epoch 42/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 0.1538\n",
      "Epoch 43/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 0.1466\n",
      "Epoch 44/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 0.1467\n",
      "Epoch 45/50\n",
      "248/248 [==============================] - 21s 84ms/step - loss: 0.1453\n",
      "Epoch 46/50\n",
      "248/248 [==============================] - 16s 63ms/step - loss: 0.1498\n",
      "Epoch 47/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 0.1489\n",
      "Epoch 48/50\n",
      "248/248 [==============================] - 16s 63ms/step - loss: 0.1568\n",
      "Epoch 49/50\n",
      "248/248 [==============================] - 15s 60ms/step - loss: 0.1517\n",
      "Epoch 50/50\n",
      "248/248 [==============================] - 21s 85ms/step - loss: 0.1376\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7fed9c0a2a20>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(1, None)]               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (1, None, 300)            2274300   \n",
      "_________________________________________________________________\n",
      "unified_lstm_3 (UnifiedLSTM) (1, None, 800)            3523200   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (1, None, 7581)           6072381   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (1, None, 7581)           57479142  \n",
      "=================================================================\n",
      "Total params: 69,349,023\n",
      "Trainable params: 69,349,023\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# for prediction, batch size has to be changed\n",
    "# So reload the model and set shape to [1, None]\n",
    "\n",
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]),)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_gen(model, start_string, freedom=1.0, num_generate=1000):\n",
    "    \"\"\"\n",
    "    generate text with the trained model\n",
    "    \n",
    "    start_string (STR):  Basis for the model to start prediction on. \n",
    "    freedom (FLOAT): Multiplier for predictions. The lower it is the lower the impact of predictive variance\n",
    "    num_generate (INT): Desired text length\n",
    "    \"\"\"\n",
    "    \n",
    "    text_generated = []\n",
    "    \n",
    "\n",
    "    # vectorization of starting string\n",
    "    input_eval = [word2idx[s] for s in tokenize(start_string)]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    model.reset_states()\n",
    "    \n",
    "    \n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        \n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        \n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / freedom\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(' ') # spacing\n",
    "        text_generated.append(idx2word[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heidi 37 ) agenten auto \" dietmar ! agency 1962 dau \n",
      " 07 1991 ! amigos 02 19 09 favoritin alvaro nachrichtenagentur 120 comeback hausarbeit \n",
      " 2008 110 . guardians +++ #metoo abgesagte 05 \n",
      " 1933 0  0  #s04tsg 75 \" abwehrspieler ! all \n",
      " 1994 mio kündigt jedem 2015 37 1 2001  94 #s04tsg 1837 \" abspeck ! 54  , \n",
      " besorgt 03 5 alleine \n",
      " bericht ! 62 02 diverser - zufolge groß 1916 ( 53  ( besser ! achtelfinal 8 06 13 #metoo \n",
      " 100 25  0  #s04tsg 75 \" 21\n"
     ]
    }
   ],
   "source": [
    "print(text_gen(model, start_string=u\"Heidi\", freedom=2, num_generate=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "513"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['knnte']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "BEGIN EPISODE [The scene opens on a dining the MRI scans and a blonde cough. Thirteen is standing behind her.]\n",
    " CUT TO:\n",
    " [Diagnostics Office. Day. The team is standing by the table in the courtroom. He sits back at the bedroom doorway]\n",
    " Vicodin administerately seriously. As soon as she holds on him, until he comes out of the Room and starts drilling it to him]\n",
    " Cuddy: You want him to take the webst now that I found the adjoining to someone's every \n",
    " Cuddy: You rather you're gonna have to give you a prescription about you, and I said you weren't right, if you transfer your case back from me, I'd be dead in a hospital. You can still get you scared, you're considering this out. People set up all the way back to the fact that it was inactive esperital in a canial dating again.\n",
    " House: Anything else?\n",
    " Foreman: Yeah, that would be idiots the way you two said it was never been in here and the recording studio that people in his shirt and white evolutionary symptoms. The clot could be a manifest rash \n",
    " Bill: We have to use it with him, we're sure it was his brain. [to Thirteen) You guys won't fire Chase.\n",
    " Cuddy: So what do we do? Tell you to take your pages for that disease?\n",
    " Melanie: Excuse me, [laughs] I know. I didn't know that you were worried about you.\n",
    " [House starts to walk away.]\n",
    " (Cut to Cameron and Chase in the conference room]\n",
    " Thirteen: You were wrong. The one that caused his car important aneurysm in the waiting room and House use it and down and sees House like him died]\n",
    " Foley: (getting the sharp panicking) Irene walking towards the classroom.]\n",
    " [Cut to a street coffired and releases the sample in the lobby.\n",
    " Cameron: The body doesn't work, they weren't all sure that work for. That doesn't mean she didn't comple here who was here. But I hear you were still alive. [All the walls look confused] You can't make sense go from those two people. You miss me right now if you're wrong with me, I say what I should try to teach you if you weren't going to do, I wanted to watch you changed your motorcycle, Dr. Foreman could’ve confronted it.\n",
    " Foreman: Yeah, but we didn't come back to your room for the bathroom because you think she was never the only mean something else which you caused. That wasn't that alone. [House takes it]\n",
    " Cuddy: The guy who was pretty much would be bacterial infection and a trache.\n",
    " [Cut to House and the team littering what is there as House and grin wheels Cuddy on the couch with his cane on a table in front of him. He slowly turns toward the refrigerator]\n",
    " [Cut to House walking into Bertingly athe in discretionances. Cuddy smiles and looks down, and throws up glass flashlight at Kenny’s face, then pauses and looks at the parents, he finishes writing the bottle of water on himself and collapses onto the floor, presents to people toward the office where Cuddy stands outside the door in front of the MRI, MRI made me someone paperwork and everyone stands beside him, everyone follows.]\n",
    " [Cut to Ducklings in the conference room. Cuddy is sitting at his desk in a chair while it looks around at the back of the bed)\n",
    " Michelle: If you're gonna have to go home soon enough.\n",
    " [Cut to House and Ducklings trying to stay him back and getting ready to dig down.]\n",
    " [Cut to the hallway. The team starts to walk away]\n",
    " Cuddy: What the hell is something to you?\n",
    " Cuddy: You don't have a chance.\n",
    " House: You don't want me to take the case you can\n",
    " [He tries to pull the eegisting up around to find a radiology video.\n",
    " Chase: We can do that on it. And get a blood tests were normal.\n",
    " Young Donny: Chase and Mark are you actually warn mother's clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
