{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# GLOVE download reference https://github.com/guillaume-chevalier/GloVe-as-a-TensorFlow-Embedding-Layer/blob/master/README.md\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from datetime import date\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense ,Dropout,LSTM, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-alpha0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Name  Dimension                     Corpus VocabularySize  \\\n",
      "2          fastText(en)        300                  Wikipedia           2.5M   \n",
      "11         GloVe.6B.50d         50  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "12        GloVe.6B.100d        100  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "13        GloVe.6B.200d        200  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "14        GloVe.6B.300d        300  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "15       GloVe.42B.300d        300          Common Crawl(42B)           1.9M   \n",
      "16      GloVe.840B.300d        300         Common Crawl(840B)           2.2M   \n",
      "17    GloVe.Twitter.25d         25               Twitter(27B)           1.2M   \n",
      "18    GloVe.Twitter.50d         50               Twitter(27B)           1.2M   \n",
      "19   GloVe.Twitter.100d        100               Twitter(27B)           1.2M   \n",
      "20   GloVe.Twitter.200d        200               Twitter(27B)           1.2M   \n",
      "21  word2vec.GoogleNews        300          Google News(100B)           3.0M   \n",
      "\n",
      "      Method Language    Author  \n",
      "2   fastText  English  Facebook  \n",
      "11     GloVe  English  Stanford  \n",
      "12     GloVe  English  Stanford  \n",
      "13     GloVe  English  Stanford  \n",
      "14     GloVe  English  Stanford  \n",
      "15     GloVe  English  Stanford  \n",
      "16     GloVe  English  Stanford  \n",
      "17     GloVe  English  Stanford  \n",
      "18     GloVe  English  Stanford  \n",
      "19     GloVe  English  Stanford  \n",
      "20     GloVe  English  Stanford  \n",
      "21  word2vec  English    Google  \n"
     ]
    }
   ],
   "source": [
    "import chakin\n",
    "\n",
    "import json\n",
    "\n",
    "from collections import defaultdict\n",
    "import zipfile\n",
    "chakin.search(lang='English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CHAKIN_INDEX = 13\n",
    "NUMBER_OF_DIMENSIONS = 25\n",
    "SUBFOLDER_NAME = \"GloVe.6B.200d\"\n",
    "\n",
    "DATA_FOLDER = \"embeddings\"\n",
    "ZIP_FILE = os.path.join(DATA_FOLDER, \"{}.zip\".format(SUBFOLDER_NAME))\n",
    "ZIP_FILE_ALT = \"glove\" + ZIP_FILE[5:]  # sometimes it's lowercase only...\n",
    "UNZIP_FOLDER = os.path.join(DATA_FOLDER, SUBFOLDER_NAME)\n",
    "GLOVE_FILENAME = os.path.join(UNZIP_FOLDER, \"{}.txt\".format(SUBFOLDER_NAME))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings already downloaded.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(ZIP_FILE) and not os.path.exists(UNZIP_FOLDER):\n",
    "    # GloVe by Stanford is licensed Apache 2.0: \n",
    "    #     https://github.com/stanfordnlp/GloVe/blob/master/LICENSE\n",
    "    #     http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "    #     Copyright 2014 The Board of Trustees of The Leland Stanford Junior University\n",
    "    print(\"Downloading embeddings to '{}'\".format(ZIP_FILE))\n",
    "    chakin.download(number=CHAKIN_INDEX, save_dir='./{}'.format(DATA_FOLDER))\n",
    "else:\n",
    "    print(\"Embeddings already downloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings already extracted.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(UNZIP_FOLDER):\n",
    "\n",
    "    with zipfile.ZipFile('embeddings/glove.6B.zip',\"r\") as zip_ref:\n",
    "        print(\"Extracting embeddings to '{}'\".format(UNZIP_FOLDER))\n",
    "        zip_ref.extractall(UNZIP_FOLDER)\n",
    "else:\n",
    "    print(\"Embeddings already extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a class that looks up embeddings\n",
    "\n",
    "\n",
    "# Not actually necessary , as the keras implementation takes embedded matrix input natively\n",
    "#class PretrainedEmbedding(tf.keras.layers.Layer):\n",
    "#    \"\"\"Embedding layer taking pretrained embeddings\"\"\"##\n",
    "\n",
    "#    def __init__(self, embeddings, rate=0.1, **kwargs):\n",
    "#        \"\"\"\"Instantiate the layer using a pre-defined embedding matrix.\"\"\"\n",
    "#        super().__init__(**kwargs)\n",
    "#        self.embeddings = tf.constant(embeddings)\n",
    "#        # if you want to add some dropout (or normalization, etc.)\n",
    "#        #self.dropout = tf.keras.layers.Dropout(rate=rate)\n",
    "\n",
    "#    def call(self, inputs, training=None):\n",
    "#        \"\"\"Embed some input tokens and optionally apply dropout.\"\"\"\n",
    "#        output = tf.nn.embedding_lookup(self.embeddings, inputs)\n",
    "#        return output #self.dropout(output, training=training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of unwanted characters\n",
    "badlist = pd.read_csv('data/badlist', header=None)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add spaces to certain special characters\n",
    "unspaced = r'[\\[\\]\\(\\)\\.\\,\\/\\?\\-\\!\\\"\\;|:]'\n",
    "spaced =  ' \\g<0> '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_line(line):\n",
    "    \"\"\"\n",
    "    remove stray characters\n",
    "    \"\"\"\n",
    "    for badchar in badlist:\n",
    "        if badchar in line:\n",
    "            line = re.sub(badchar,'',line)\n",
    "            \n",
    "    line = re.sub(unspaced, spaced, line)  \n",
    "    line = re.sub('”','',line)\n",
    "    line = re.sub('’' ,\"'\", line)\n",
    "    line = re.sub(\"'s\" ,\" 's\", line)\n",
    "    line = re.sub(\"\\n\" ,\" \\n\", line)\n",
    "    \n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the raw text files and clean the lines\n",
    "\n",
    "START_STRING = 'BEGIN EPISODE'\n",
    "\n",
    "all_episodes_by_sentence = []\n",
    "\n",
    "for element in os.listdir('house/'):\n",
    "    if 'clean' in element:\n",
    "        with open('house/'+element) as in_raw:\n",
    "            # start token\n",
    "            all_episodes_by_sentence.append(START_STRING)\n",
    "            for (i, line) in enumerate(in_raw):\n",
    "                all_episodes_by_sentence.append(clean_line(line) )\n",
    "        \n",
    "            # end token\n",
    "            all_episodes_by_sentence.append('END EPISODE\\n\\n')\n",
    "            all_episodes_by_sentence.append('------------------------------------------\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    \"\"\"\n",
    "    simple tokenization keeping the line seperators etc.\n",
    "    \"\"\"\n",
    "    return sentence.lower().split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine text to create a single string for sliceshifting\n",
    "word_dict = {}\n",
    "used_words = []\n",
    "text = []\n",
    "for sent in all_episodes_by_sentence:\n",
    "    for word in tokenize(sent):\n",
    "\n",
    "        if not word in word_dict:\n",
    "            word_dict[word] = 0\n",
    "            used_words.append(word)\n",
    "        word_dict[word] +=1\n",
    "        text.append(word)\n",
    "#text = [tokenize(sent) for sent in all_episodes_by_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26547"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of words\n",
    "len(used_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if we can reduce the number of words\n",
    "data_quant = pd.DataFrame.from_dict(word_dict, orient='index').sort_values(by=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26547, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_quant.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>transmitter</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>revelations</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tagline</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grating</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reclaimed</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinc</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seamen</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intertwines</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "transmitter  1\n",
       "revelations  1\n",
       "tagline      1\n",
       "feature      1\n",
       "grating      1\n",
       "...         ..\n",
       "reclaimed    1\n",
       "190          1\n",
       "clinc        1\n",
       "seamen       1\n",
       "intertwines  1\n",
       "\n",
       "[5000 rows x 1 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_quant.tail(5000) # looking clean, but many words with only one occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7555, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only 7401 words occur more than 6 times\n",
    "data_quant.loc[data_quant[0] > 6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_vocabulary = data_quant.loc[data_quant[0] > 6].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preformat to test if this is enough\n",
    "word_dict = {}\n",
    "used_words = []\n",
    "text = []\n",
    "for sent in all_episodes_by_sentence:\n",
    "    for word in tokenize(sent):\n",
    "        if word in allowed_vocabulary:\n",
    "            if not word in word_dict:\n",
    "                word_dict[word] = 0\n",
    "                used_words.append(word)\n",
    "            word_dict[word] +=1\n",
    "            text.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse dictionary for word_count\n",
    "word2idx = {}\n",
    "idx2word= {}\n",
    "for idx, word in enumerate(allowed_vocabulary):\n",
    "    word2idx[word] = idx\n",
    "    idx2word[idx] =  word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding from file\n",
    "embedded_words = {}\n",
    "with open('embeddings/GloVe.6B.200d/glove.6B.300d.txt', 'r' ) as glove_file:\n",
    "    for (i, line) in enumerate(glove_file):\n",
    "        split = line.split(' ')\n",
    "            \n",
    "        word = split[0]\n",
    "        representation = split[1:]\n",
    "        representation = np.array([float(val) for val in representation] )\n",
    "        \n",
    "        embedded_words[word] = representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an embedding matrix from only the known words\n",
    "EMBEDDING_MATRIX = np.zeros((len(word2idx.items()) + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, vec in embedded_words.items():\n",
    "    if word in word2idx:\n",
    "        EMBEDDING_MATRIX[word2idx[word]] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BEGIN EPISODE',\n",
       " \" [ Open on a House 's face .  His eyes are closed .  The picture is not quite in color ,  but it 's not black and white either .  Radiohead 's No Surprises plays .  ]  \\n\",\n",
       " \" [ House opens his eyes .  He 's lying on a twin bed on the left side of a cell - like room at Mayfield .  He 's wearing a gray t - shirt .  There is a stainless steel basin on the tiled floor near his head .  On the opposite wall there is a single window .  Next to it a metal sink is bolted into the wall .  Another stainless basin is on the floor below the sink .  ]  \\n\",\n",
       " \" [ Cut to House opening his eyes .  The color has returned to normal .  Everything is quiet .  He isn't restrained any longer .  He touches his thigh briefly then sits up .  He limps to the window ,  holding his leg for support .  ]  \\n\",\n",
       " ' [ Cut to a suitcase dropping on the bed .  House packs his clothes and zips the valise .  ]  \\n',\n",
       " ' [ Cut to House walking down the hall with his cane in his right hand and his suitcase in his left .  He switches the case to his right so he can swipe a knit cap off a patient in a wheelchair .  ]  \\n',\n",
       " \" [ Cut to House ,  wearing the cap ,  at the nurses' station .  ]  \\n\",\n",
       " 'House :  Dry heaves are gone ,  so am I .  \\n',\n",
       " \"Valdez :  I'll check with the doctor .  \\n\",\n",
       " \"House :  No ,  no ,  no ,  no .  I'm here voluntarily .  Just got to check with me .  \\n\"]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original text, first 10 sentences\n",
    "all_episodes_by_sentence[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"begin episode  [ open on a house 's face .  his eyes are closed .  the picture is not quite in color ,  but it 's not black and white either .  's no surprises plays .  ]  \\n  [ house opens his eyes .  he 's lying on a twin bed on the left side of a cell - like room at mayfield .  he 's wearing a gray t - shirt .  there is a steel basin on the floor near his head .  on the opposite wall there is a single window .  next to it a metal sink is into the wall .  another basin is on the floor below the sink .  ]  \\n  [ cut to house opening his eyes .  the color has returned to normal .  everything is quiet .  he isn't any longer .  he touches his thigh briefly then sits up .  he limps to the window ,  holding his leg for support .  ]  \\n  [ cut to a suitcase dropping on the\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(text[0:200]) # missing a few proper nouns like Radiohead, but this should do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7555 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(allowed_vocabulary)\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "# Here we are acutally using the pretrained words, so this is not needed\n",
    "#word2idx = {u:i for i, u in enumerate(vocab)}\n",
    "#idx2word = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([675, 714,   0, ..., 280, 821, 820])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int = np.array([word2idx[c] for c in text])\n",
    "text_as_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length sentence we want for a single input in words\n",
    "seq_length = 20  # 20 words traininsdata\n",
    "examples_per_epoch = len(text)//(seq_length+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([675, 714,   0, ..., 280, 821, 820])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training examples / targets\n",
    "word_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sequences from character dataset\n",
    "sequences = word_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_shift_input(segment):\n",
    "    \"\"\"\n",
    "    Creates the teaching data by shifting the training data on off to create labeled data\n",
    "    \"\"\"\n",
    "    input_segment = segment[:-1]\n",
    "    target_segment = segment[1:]\n",
    "    return input_segment, target_segment\n",
    "\n",
    "dataset = sequences.map(split_shift_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dataset as prebatched\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "# shuffle the dataset and set batch size\n",
    "dataset = dataset.shuffle(10000).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Length of the vocabulary\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# hyperparameters\n",
    "\n",
    "# embedding dimension\n",
    "embedding_dim = 300\n",
    "\n",
    "# RNN units\n",
    "rnn_units = 850\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size,  dropout = False):\n",
    "    \n",
    "    i = Input(shape=(None,), batch_size=batch_size, dtype=tf.int64 )\n",
    "    # use pretrained embedding now\n",
    "    \n",
    "    # feed the embedding matrix into the Embedding layer, set trainable to False\n",
    "    x = Embedding(vocab_size, embedding_dim,weights=EMBEDDING_MATRIX,  trainable=False)(i)\n",
    "    \n",
    "    if dropout:\n",
    "        dropout = 0.2\n",
    "    else:\n",
    "        dropout = 0.0\n",
    "    \n",
    "    x = LSTM(rnn_units, \n",
    "             return_sequences=True,\n",
    "             dropout=dropout,\n",
    "             stateful=True)(x)\n",
    "    \n",
    "    x = Dense(vocab_size)(x)\n",
    "    \n",
    "    model = Model(i,x)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(20, None)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (20, None, 300)           2266500   \n",
      "_________________________________________________________________\n",
      "unified_lstm_1 (UnifiedLSTM) (20, None, 850)           3913400   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (20, None, 7555)          6429305   \n",
      "=================================================================\n",
      "Total params: 12,609,205\n",
      "Trainable params: 10,342,705\n",
      "Non-trainable params: 2,266,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# simple model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    \"\"\"\n",
    "    Define loss function \n",
    "    \"\"\"\n",
    "    return sparse_categorical_crossentropy(labels, logits, from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss =loss)# loss='sparse_categorical_crossentropy' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "today = date.today()\n",
    "\n",
    "checkpoint_dir = './pretrained_word_training_checkpoints_{today}'.format(today=today)\n",
    "\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "# define callbacks\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4501/4501 [==============================] - 766s 170ms/step - loss: 4.0311\n",
      "Epoch 2/50\n",
      "1914/4501 [===========>..................] - ETA: 7:18 - loss: 3.6434"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7fb2eb78e4e0>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(1, None)]               0         \n",
      "_________________________________________________________________\n",
      "pretrained_embedding_6 (Pret (1, None, 300)            0         \n",
      "_________________________________________________________________\n",
      "unified_lstm_6 (UnifiedLSTM) (1, None, 1000)           5204000   \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (1, None, 7401)           7408401   \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (1, None, 7401)           54782202  \n",
      "=================================================================\n",
      "Total params: 67,394,603\n",
      "Trainable params: 67,394,603\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# for prediction, batch size has to be changed\n",
    "# So reload the model and set shape to [1, None]\n",
    "\n",
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1, embedding_matrix= embedding_matrix)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]),)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_gen(model, start_string=START_STRING, freedom=1.0, num_generate=1000):\n",
    "    \"\"\"\n",
    "    generate text with the trained model\n",
    "    \n",
    "    start_string (STR):  Basis for the model to start prediction on. \n",
    "    freedom (FLOAT): Multiplier for predictions. The lower it is the lower the impact of predictive variance\n",
    "    num_generate (INT): Desired text length\n",
    "    \"\"\"\n",
    "    \n",
    "    text_generated = []\n",
    "    \n",
    "\n",
    "    # vectorization of starting string\n",
    "    input_eval = [word2idx[s] for s in tokenize(start_string)]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    model.reset_states()\n",
    "    \n",
    "    \n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        \n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        \n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / freedom\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(' ') # spacing\n",
    "        text_generated.append(idx2word[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN EPISODE the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "print(text_gen(model, start_string=u\"BEGIN EPISODE\", freedom=1, num_generate=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "BEGIN EPISODE [The scene opens on a dining the MRI scans and a blonde cough. Thirteen is standing behind her.]\n",
    " CUT TO:\n",
    " [Diagnostics Office. Day. The team is standing by the table in the courtroom. He sits back at the bedroom doorway]\n",
    " Vicodin administerately seriously. As soon as she holds on him, until he comes out of the Room and starts drilling it to him]\n",
    " Cuddy: You want him to take the webst now that I found the adjoining to someone's every \n",
    " Cuddy: You rather you're gonna have to give you a prescription about you, and I said you weren't right, if you transfer your case back from me, I'd be dead in a hospital. You can still get you scared, you're considering this out. People set up all the way back to the fact that it was inactive esperital in a canial dating again.\n",
    " House: Anything else?\n",
    " Foreman: Yeah, that would be idiots the way you two said it was never been in here and the recording studio that people in his shirt and white evolutionary symptoms. The clot could be a manifest rash \n",
    " Bill: We have to use it with him, we're sure it was his brain. [to Thirteen) You guys won't fire Chase.\n",
    " Cuddy: So what do we do? Tell you to take your pages for that disease?\n",
    " Melanie: Excuse me, [laughs] I know. I didn't know that you were worried about you.\n",
    " [House starts to walk away.]\n",
    " (Cut to Cameron and Chase in the conference room]\n",
    " Thirteen: You were wrong. The one that caused his car important aneurysm in the waiting room and House use it and down and sees House like him died]\n",
    " Foley: (getting the sharp panicking) Irene walking towards the classroom.]\n",
    " [Cut to a street coffired and releases the sample in the lobby.\n",
    " Cameron: The body doesn't work, they weren't all sure that work for. That doesn't mean she didn't comple here who was here. But I hear you were still alive. [All the walls look confused] You can't make sense go from those two people. You miss me right now if you're wrong with me, I say what I should try to teach you if you weren't going to do, I wanted to watch you changed your motorcycle, Dr. Foreman could’ve confronted it.\n",
    " Foreman: Yeah, but we didn't come back to your room for the bathroom because you think she was never the only mean something else which you caused. That wasn't that alone. [House takes it]\n",
    " Cuddy: The guy who was pretty much would be bacterial infection and a trache.\n",
    " [Cut to House and the team littering what is there as House and grin wheels Cuddy on the couch with his cane on a table in front of him. He slowly turns toward the refrigerator]\n",
    " [Cut to House walking into Bertingly athe in discretionances. Cuddy smiles and looks down, and throws up glass flashlight at Kenny’s face, then pauses and looks at the parents, he finishes writing the bottle of water on himself and collapses onto the floor, presents to people toward the office where Cuddy stands outside the door in front of the MRI, MRI made me someone paperwork and everyone stands beside him, everyone follows.]\n",
    " [Cut to Ducklings in the conference room. Cuddy is sitting at his desk in a chair while it looks around at the back of the bed)\n",
    " Michelle: If you're gonna have to go home soon enough.\n",
    " [Cut to House and Ducklings trying to stay him back and getting ready to dig down.]\n",
    " [Cut to the hallway. The team starts to walk away]\n",
    " Cuddy: What the hell is something to you?\n",
    " Cuddy: You don't have a chance.\n",
    " House: You don't want me to take the case you can\n",
    " [He tries to pull the eegisting up around to find a radiology video.\n",
    " Chase: We can do that on it. And get a blood tests were normal.\n",
    " Young Donny: Chase and Mark are you actually warn mother's clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
